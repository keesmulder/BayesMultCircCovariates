\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{apacite}
\usepackage{hyperref}
\hypersetup{
pdfauthor={Kees Mulder},
colorlinks=true,
citecolor=blue,
linkcolor=red,
urlcolor=blue
}

\usepackage{natbib}

\title{An Introduction to Bayesian Circular GLM}
\author{Kees Mulder}

% Space formula's more.
\setlength{\jot}{14pt}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

\begin{document}









\maketitle
\tableofcontents
\section{Introduction}

Our Bayesian circular regression (GLM) model attempts to predict some circular outcome $\boldsymbol\theta$ by means of a set of linear predictors/covariates $\boldsymbol{X}$ through coefficients $\boldsymbol\beta$, employing of some link function $g(x)$. Specifically:

\begin{itemize}

\item $\boldsymbol\theta = \theta_1, \dots, \theta_n$, where $n$ is the total sample size. Observations are given subscript $i$.
\item $\boldsymbol{X}$ is an $n \times K$ matrix of predictors, where each observation $x_{ik}$ is the score for observation $i$ on predictor $k$. One $\boldsymbol{x}_{i \cdot}$, a $K$ vector,  contains the set of predictors for each observation. Splitting up the by predictor $\boldsymbol{x}_{\cdot k}$ instead, each of the predictors is centred around its own mean, that is $\bar{\boldsymbol{x}}_{\cdot k} = 0$. The predictors must be centred, because the link function does not need to have the same shape across the real line, that is, the link function is not necessarily invariant under adding constants. In theory, we could estimate an additional parameter $c$ which forces the model to use some specific part the link function, but it seems unlikely that this will be a good fit to any real-life datasets.
\item $\boldsymbol\beta = \beta_1, \dots, \beta_K$ is a $K$ vector of coefficients, each denoted by $\beta_k$.
\item $ \beta_0 \in [-\pi, \pi) $ is an offset parameter, analogous to an intercept in linear regression with centred predictors. As long as the predictors are centred, it roughly coincides with $\bar\theta$, the grand mean of the data. Because of this, it is sometimes simply called $\mu$, for example in \citet{fisher1992regression}. However, to emphasize the possible differences between it and the grand mean $\mu$, the notation $\beta_0$ is preferred here.
\item $\kappa$ is the residual concentration. As of now, this is not being predicted and thus equal for all observations.

\end{itemize}

The model supposes each observation $\theta_i$ to be distributed according to a von Mises distribution $$ \mathcal{VM}(\theta_i \vert \hat\mu_i, \kappa),$$  $$\hat\mu_i = \beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i),$$ where $g(\cdot)$ is the \textit{link function} (see Section \ref{link}). Thus,

\begin{align*}
\text{Pr}(\Theta_i = \theta_i) &=  \mathcal{VM}(\theta_i \vert \beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i), \kappa) \\
&= \left\lbrace 2 \pi I_0(\kappa) \right\rbrace^{-1} \exp \left\lbrace \kappa \cos \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right]  \right\rbrace. $$
\end{align*}

\section{Link function} \label{link}

The link function maps the range of $\boldsymbol{x}_i^T \boldsymbol\beta$, which is $(-\infty, \infty)$, that is, $\mathbb{R}^1,$ to (a subset of) the circular sample space $\mathbb{S}^1$ in some way. So:

\begin{align}
\boldsymbol{x}_i^T \boldsymbol\beta &\in \mathbb{R}^1 \\
g(\boldsymbol{x}_i^T \boldsymbol\beta) &\in \mathbb{S}^1.
\end{align}

Most applications use $$g(x) = 2 \tan^{-1}(x),$$ because for this link function $g(0) = 0,$ and because it maps the real line to $[-\pi, \pi).$ The first property ensures that the grand mean is near the data for centred predictors $X$, the second makes sure that the prediction can pass through observations on the entire circle. This link function is shown in figure \ref{linkfunc}.

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\linewidth]{figure/Link} 

}



\end{knitrout}

\caption{Link function $g(x) = 2 \tan^{-1}(x).$}
\label{linkfunc}
\end{figure}

\newpage

\section{MCMC Sampling}

\subsection{Algorithm}

The MCMC algorithm is implemented as follows:

First, set starting values for $\boldsymbol\beta, \beta_0,$ and $\kappa$. Choose some bandwith $b_k$, for each $\beta_k$. Then, for iteration $j = 1, \dots, Q$:

\begin{enumerate}

\item Obtaining $\beta_0$:
\begin{itemize}
\item Compute $\psi_i = \theta_i - g(\boldsymbol\beta^T \boldsymbol{x}_i),$ for each $i = 1, \dots, n. $
\item Draw a value for $\beta_0$ from $\mathcal{VM}(\bar\psi, R_{\psi} \kappa).$
\end{itemize}

\item Obtaining $\boldsymbol\beta$. For each coefficient $k = 1, \dots, K$:
\begin{itemize}
\item Generate a candidate $\beta_k^{(can)} = \beta_k^{(cur)} + \tau,$ where $\tau$ is a uniform r.v. from $U(-b_k, b_k)$. This proposal is symmetric, which simplifies the MH-step.
\item Calculate the log MH ratio as
\begin{align*}
A_{\beta_k} = ~& \ln L \left( \beta_{k}^{(can)} \vert \beta_0^{(cur)}, \boldsymbol\beta_{-(k)}^{(cur)}, \kappa^{(cur)}, \boldsymbol\theta, \boldsymbol{X} \right) + \ln \text{Prior}_{\beta_k}\left(\beta_{k}^{(can)}\right) \\
- & \ln L \left( \beta_{k}^{(cur)} \vert \beta_0^{(cur)}, \boldsymbol\beta_{-(k)}^{(cur)}, \kappa^{(cur)}, \boldsymbol\theta, \boldsymbol{X} \right) - \ln \text{Prior}_{\beta_k}\left(\beta_{k}^{(cur)}\right),
\end{align*}
\item Draw a new value $u$ from $U(0,1)$
\item If $ A_{\beta_k} > \ln u $, set $\beta_k^{(cur)} = \beta_k^{(can)}$. Elsewise, remain at $\beta_k^{(cur)}$.
\end{itemize}

\item Obtaining $\kappa$:
\begin{itemize}
\item Draw a candidate $\kappa^{(can)}$ from $\chi^2(\kappa^{(can)} \vert \kappa^{(cur)})$.
\item Calculate the log MH ratio as
\begin{align*}
A_\kappa = ~& \ln L \left( \kappa^{(can)} \vert \beta_0^{(cur)}, \boldsymbol\beta^{(cur)}, \boldsymbol\theta, \boldsymbol{X} \right) + \ln \chi^2 \left(\kappa^{(cur)} \vert \kappa^{(can)}\right) \\
- & \ln L \left( \kappa^{(cur)} \vert \beta_0^{(cur)}, \boldsymbol\beta^{(cur)}, \boldsymbol\theta, \boldsymbol{X} \right) - \ln \chi^2 \left(\kappa^{(can)} \vert \kappa^{(cur)}\right),
\end{align*}
\item Draw a new value $u$ from $U(0,1)$.
\item If $A_\kappa > \ln u $, set $\kappa^{(cur)} = \kappa^{(can)}$. Elsewise, remain at $\kappa^{(cur)}$.
\end{itemize}

\end{enumerate}


\subsection{Properties}

Some properties of this sampler:

\begin{itemize}
\item The intercept $\beta_0$ is drawn from the von Mises distribution directly because the conditional distribution of $\beta_0$ has this form. This is shown in Section \ref{beta0}.
\item The prior for $\beta_0$ is uniform across the circle, and thus omitted. In theory, we can add an informative prior, preferably a conjugate one through \citet{guttorp1988finding}. However, considering $\beta_0$ as the intercept, it may be difficult to formulate prior knowledge on this subject. Also, the bounded sample space of circular data makes uninformative priors for the location straightforward.
\item The uniform proposals for $\beta$ are most likely far from optimal, but the likelihood of $\beta$ is not similar to any known density (see Section \ref{betashape}). For some attempts at finding the location of the currently most likely value for some $\beta_k$, see Section \ref{betamle}. The location, however, is not enough, as we need a proposal with a similar spread to the posterior of $\beta_k$ to obtain really good acceptance rates.
\item The sampler can very easily become stuck in a local maximum for one or more of the $\beta$'s, given inappropriate starting values. This is a major problem of the sampler, as it is not straightforward to determine what starting values to use. This issue is explored in Section \ref{betashape}. This problem is currently not solved.
\item An MH-step for $\kappa$ is currently implemented for the sake of simplicity, but \citet{forbes2014fast} is the preferred method as it is faster and shows less autocorrelation \citep{mulder2014extending}. It would use $$ - \frac{R_\psi \cos(\beta_0 - \bar\psi)}{n} $$ in place of what is confusingly called $\beta_0$ in \citet{forbes2014fast}.
\end{itemize}

\subsection{Example run}

Some data was generated with:
\begin{gather*}
n = 100 \\
\beta_0 = \pi, \\
\beta_1 = 3, \\
\beta_2 = 6, \\
\kappa = 20, \\
\text{so that} \\
\theta_i = \beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i) + \epsilon, \\
\text{with} \\
\epsilon \sim \mathcal{VM}(0, \kappa)
\end{gather*}

Starting values were
\begin{align*}
\beta_0 &= \pi, \\
\beta_1 &= 0, \\
\beta_2 &= 0, \\
\kappa &= 1.
\end{align*}

The bandwith was .5 for both $\beta$'s. Below, the first 1000 iterations are shown.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/examplerun} 

}



\end{knitrout}





\newpage

\section{Shape of the likelihood of $\beta$} \label{betashape}

In this section, we will examine the likelihood of $\beta$ in the simple case where there is only a single predictor, for a dataset with $n=100$.

First, a simple case. Data was generated with
\begin{align*}
n &= 100 \\
\beta_0 &= \pi, \\
\beta_1 &= 0.2, \\
\kappa &= 20.
\end{align*}

The Likelihood-function is given by $L(\beta_0, \beta_1, \kappa \vert \boldsymbol{X}, \boldsymbol{\theta})$. $\kappa$ is only multiplicative, which is irrelevant for its shape, so it will be disregarded here. Thus, we can make a 3D plot which shows what the joint log-likelihood of $\beta_0$ and $\beta_1$ looks like. The Likelihood-function then, has the following shape:
\begin{figure}[h!]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/plot3d} 

}



\end{knitrout}

\end{figure}

Essentially, little problems seem apparent from this likelihood. However, we see some problems arise when we inspect the log-likelihood.

The log-likelihood is $\ell(\beta_0, \beta_1, \kappa \vert \boldsymbol{X}, \boldsymbol{\theta})$. It is shown as:


\begin{figure}[h!]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/plot3dlog} 

}



\end{knitrout}

\end{figure}

It is apparent from this picture that the joint likelihood is not globally concave. In the corners, an MCMC algorithm would move towards a global minimum with $\beta_0$ moving to 0 or $2\pi$ and $\beta_1$ moving towards $-\infty$ or $\infty$. In the coming pages, several examples are shown, showing why this likelihood has this form.

\newpage

\begin{figure}[t!]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/plot1} 

}



\end{knitrout}

\end{figure}

Here, data was generated with
\begin{align*}
n &= 100 \\
\beta_0 &= \pi, \\
\beta_1 &= 0.2, \\
\kappa &= 20.
\end{align*}

In this plot, the top left panel shows the data on the circle. The top right panel shows the relation between the covariate $X$ and dependent circular variable $\theta$, as well as the mean direction, which we can see differs from $\beta_0$. The bottom left shows the likelihood function, conditional for one specific value for $\beta_0$ which is denoted in the title. It also shows four colored lines that denote specific values of $\beta_1$, where the green line is the maximum found with the shown interval, red is the minimum, and the two shades of blue simply show -5 and 5. The bottom right plot shows the resulting predictions from these four possible values of $\beta_1$, that is, it shows $\hat\mu_i = \beta_0 + g(\beta_1 x_i)$ for each of the possible values for $\beta_1$. The purple dashed line shows the current $\beta_0$, which is not necessarily the same as the one under which the data was generated. Grey dotted lines show a range of size $2\pi$ of possible predictions. Note that the viewpoint is slightly shifted to be centred around the current $\beta_0$ and that duplicate points for some of the observed $\theta$ are shown. Also not that because $g(0) = 0$, when $x=0$, $\hat\mu_i = \beta_0 + g(0) = \beta_0$, which forces the predictions of given by any value for $\beta_1$ to pass through coordinate $(0, \beta_0)$.

In the current plot, we see a simple example of a situation where the likelihood is almost entirely concave. The green line is the highest likelihood and follows the data fairly well, while the red line is quite far from the actual data. Note that $\beta_0$ is equal to the value used in generation.

\newpage

\begin{figure}[t!]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/plot2} 

}



\end{knitrout}

\end{figure}

Here, data was again generated with
\begin{align*}
n &= 100 \\
\beta_0 &= \pi, \\
\beta_1 &= 0.2, \\
\kappa &= 20.
\end{align*}

However, we evaluate the data when the current value for $\beta_0= \beta_0^{(true)} + 1.7$. we can see that the shifted data is moving towards where the data lies on the other side of the circle as well as within the range where predictions lie. The worst prediction (red) is strikingly similar to our previous best prediction.

\newpage

\begin{figure}[t!]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/plot3} 

}



\end{knitrout}

\end{figure}


Here, data was again generated with
\begin{align*}
n &= 100 \\
\beta_0 &= \pi, \\
\beta_1 &= 0.2, \\
\kappa &= 20.
\end{align*}

When we evaluate the data when the current value for $\beta_0= \beta_0^{(true)} + \pi$,  we find asymptotes at the maximum, as the predictions tries to move towards the data.


\newpage

\begin{figure}[t!]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/plot4} 

}



\end{knitrout}

\end{figure}


Here, data was generated with
\begin{align*}
n &= 100 \\
\beta_0 &= 3, \\
\beta_1 &= 0.5, \\
\kappa &= 10.
\end{align*}

And we evaluate the likelihood at $\beta_0 = \beta_0^{(true)}$. Note how the blue line is essentially a "fair" prediction, while it is not at all the prediction we are looking for.


\newpage

\begin{figure}[t!]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/plot5} 

}



\end{knitrout}

\end{figure}


Here, data was again generated with
\begin{align*}
n &= 100 \\
\beta_0 &= 3, \\
\beta_1 &= 0.1, \\
\kappa &= 3.
\end{align*}

And we evaluate the likelihood at $\beta_0 = 0$.

One note about these plots: this problem is in essence a problem of starting values: if an MCMC chain finds itself in the main probability mass, it will most likely not stray from it. However, there may not be a straightforward way to choose these values if there are more than one or two $\beta$'s.

Possible solutions for this problem:

\begin{itemize}
\item Choose a prior, such that high values for $\beta$ are very unlikely, or in such a way that the posterior for $\beta$ can be proven to be globally concave.
\item Choose a different link function, for example $g(x) = 1.5 \tan^{-1} (x)$, which will restrict the range of the places where we can predict values. If I understand projected normal circular regression correctly, it is restricted to an arc of length $\pi$, as would be $g(x) = \tan^{-1} (x)$.
\item Choose roughly correct starting values for each $\beta$ by some procedure, such at iteratively selecting conditional maximum likelihoods as in section \ref{betamle}, or using an IRLS algorithm as in \citet{gill2010}. However, these methods themselves require starting values, so they may not solve anything.
\item Ignore the problem, pretend that all errors with the model in practice are due to 'inappropriate' starting values and thus the statisticians can not be blamed.
\end{itemize}


\newpage


\section{Conditional distribution of $\beta_0$} \label{beta0}

Here, I will show that the conditional distribution for $\beta_0$,

$$ L(\beta_0 \vert \boldsymbol\beta, \kappa, \theta, \boldsymbol{X}) \propto \exp \left\lbrace \kappa \sum_{i=1}^n \cos \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right]  \right\rbrace \propto \mathcal{VM}(\beta_0 \vert \bar\psi, R_{\psi}\kappa). $$

The proof for the conditional distribution of $\beta_0$ in the GLM closely follows the derivation for the distribution of the mean direction $\mu$ of the von Mises distribution, which shows that $L(\mu \vert \kappa, \boldsymbol\theta) \propto \mathcal{VM}(\mu \vert \bar\theta, R_{\theta} \kappa)$.

The conditional likelihood of $\beta_0$ is given by

\begin{align*}
L(\beta_0 \vert \boldsymbol\beta, \kappa, \theta, \boldsymbol{X}) & \propto \exp \left\lbrace \kappa \sum_{i=1}^n \cos \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] \right\rbrace \\
& = \exp \left\lbrace \kappa \sum_{i=1}^n \cos \left[ \beta_0 - (\theta_i - g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right]  \right\rbrace.
\end{align*}

We know that for any angle $\psi_i, i = 1, \dots, n$,

$$ C_{\psi} = \sum_{i=1}^n \cos(\psi_i), ~~ S_{\psi} = \sum_{i=1}^n \sin(\psi_i), ~~ R_{\psi} = \sqrt{C_{\psi}^2 + S_{\psi}^2}, ~~ $$

$$ \text{and} ~~ \frac{C_{\psi}}{R_{\psi}} = \cos \bar\psi, ~~ \frac{S_{\psi}}{R_{\psi}} = \sin \bar\psi, ~~ \text{where} ~~ \bar\psi = \text{atan2}(S_{\psi}, C_{\psi}). $$

Thus, setting angle $\psi_i = \theta_i - g(\boldsymbol\beta^T \boldsymbol{x}_i)$,


\begin{align*}
L(\beta_0 \vert \boldsymbol\beta, \kappa, \boldsymbol\theta, \boldsymbol{X}) & \propto \exp \left\lbrace \kappa \sum_{i=1}^n \cos ( \beta_0 - \psi_i)  \right\rbrace \\
& = \exp \left\lbrace \kappa   \left[ \cos\beta_0 \sum_{i=1}^n \cos\psi_i +  \sin\beta_0 \sum_{i=1}^n \sin\psi_i \right]  \right\rbrace \\
& = \exp \left\lbrace R_{\psi}  \kappa  \left[ \cos\beta_0 \frac{C_{\psi}}{R_{\psi}} +  \sin\beta_0 \frac{S_{\psi}}{R_{\psi}} \right]  \right\rbrace \\
& = \exp \left\lbrace R_{\psi} \kappa   \left[ \cos\beta_0 \cos{\bar\psi} +  \sin\beta_0 \sin{\bar\psi}\right]  \right\rbrace \\
& = \exp \left\lbrace R_{\psi} \kappa \cos \left( \beta_0 - \bar\psi \right)  \right\rbrace \\
& \propto \mathcal{VM}(\beta_0 \vert \bar\psi, R_{\psi} \kappa).
\end{align*}





\section{Maximum a posteriori estimates for $\boldsymbol\beta$} \label{betamle}

It is possible to simplify finding the maximum of the posterior of $\beta_k$, that is, one of the coefficients, by taking the derivative of the log-posterior, and setting it to zero. The posterior is denoted by
$$ \text{Post}(\beta_k \vert \beta_0, \boldsymbol\beta_{-(k)}, \kappa, \boldsymbol\theta, \boldsymbol{X}) = L(\beta_k \vert \beta_0, \boldsymbol\beta_{-(k)}, \kappa, \boldsymbol\theta, \boldsymbol{X}) \text{Prior}(\beta_k).$$
We will find
$$  \frac{d}{d\beta_k} \log \left[ \text{Post}(\beta_k \vert \beta_0, \boldsymbol\beta_{-(k)}, \kappa, \boldsymbol\theta, \boldsymbol{X})) \right] = \frac{d}{d\beta_k} \log \left[ L(\beta_k \vert \beta_0, \boldsymbol\beta_{-(k)}, \kappa, \boldsymbol\theta, \boldsymbol{X})\right] +  \frac{d}{d\beta_k} \log \left[ \text{Prior}(\beta_k) \right].$$

For the first part, $\frac{d}{d\beta_k} \log \left[ L(\beta_k \vert \beta_0, \boldsymbol\beta_{-(k)}, \kappa, \boldsymbol\theta, \boldsymbol{X})\right]$, this is found as follows.

\begin{align*}
L(\beta_k \vert \beta_0, \boldsymbol\beta_{-(k)}, \kappa, \boldsymbol\theta, \boldsymbol{X})  & =  \left\lbrace 2 \pi I_0(\kappa) \right\rbrace^{-n} \exp \left\lbrace \kappa \sum_{i=1}^{n} \cos \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right]  \right\rbrace  \\
& \propto \exp \left\lbrace \kappa \sum_{i=1}^{n} \cos \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right]  \right\rbrace.
\end{align*}

Then the log-likelihood

\begin{equation*}
\ell(\beta_{k} \vert \beta_0, \boldsymbol\beta_{-(k)}, \kappa, \boldsymbol\theta, \boldsymbol{X})  \propto \kappa \sum_{i=1}^{n} \cos \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right].
\end{equation*}

Now, if we take the derivative with respect to $\beta_k,$

\begin{align*}
\frac{d\ell(\beta_{k} \vert \beta_0, \boldsymbol\beta_{-(k)}, \kappa, \boldsymbol\theta, \boldsymbol{X})}{d\beta_k}
& = - \kappa \sum_{i=1}^{n} \sin \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] \frac{d}{d\beta_k} \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] \\
& = -\kappa \sum_{i=1}^{n} \sin \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] \frac{d}{d\beta_k} - g(\boldsymbol\beta^T \boldsymbol{x}_i)) \\
& = \kappa \sum_{i=1}^{n} \sin \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] \frac{d}{d\beta_k} g(\boldsymbol\beta^T \boldsymbol{x}_i) \\
& = \kappa \sum_{i=1}^{n} \sin \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] g'(\boldsymbol\beta^T \boldsymbol{x}_i) \left[  \frac{d}{d\beta_k} \boldsymbol\beta^T \boldsymbol{x}_i \right] \\
& = \kappa \sum_{i=1}^{n} \sin \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] g'(\boldsymbol\beta^T \boldsymbol{x}_i) \left[ \frac{d}{d\beta_k} \sum_{j=1}^K \beta_j x_{ij} \right] \\
& = \kappa \sum_{i=1}^{n} \sin \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] g'(\boldsymbol\beta^T \boldsymbol{x}_i) \left[ \frac{d}{d\beta_k}  \beta_k x_{ik} \right] \\
& = \kappa \sum_{i=1}^{n} \sin \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] g'(\boldsymbol\beta^T \boldsymbol{x}_i) \left[ x_{ik} \right].
\end{align*}

The optima of the posterior are then the values of $\beta_k$ for which

$$ \sum_{i=1}^{n} \sin \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] g'(\boldsymbol\beta^T \boldsymbol{x}_i) \left[ x_{ik} \right] + \frac{d}{d\beta_k} \log \left[ \text{Prior}(\beta_k) \right] = 0, $$

which can be found by means of numerical root-finding.

Note, however, that this procedure returns a set of optima, of which there may be several: minima and maxima, global and local. This happens because the posterior of $\beta_k$ is not necessarily globally concave, so there might also be minima. This means that there may be multiple points at which the derivative of the log-posterior is zero (ie. any optimum in the posterior, either maximum or minimum), so we first have to choose the estimate among the options which has the highest likelihood.


\bibliographystyle{apacite}
\bibliography{C:/Dropbox/ArchiveMastertheses/CircularData}
% \bibliography{F:/Dropbox/LiteratureCircular/CircularData}

\end{document}

