\documentclass[10pt,a4paper]{article}

\usepackage{apacite}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{hyperref}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\hypersetup{
    colorlinks=true,
    citecolor=blue,
    urlcolor=blue
}

\author{Kees Mulder}
\title{Estimation and Hypothesis Testing for a Bayesian Circular GLM}

\begin{document}

\maketitle

\tableofcontents

\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bt}{\boldsymbol{\theta}}
\newcommand{\bd}{\boldsymbol{d}}
\newcommand{\bdt}{\boldsymbol{\delta}}
\newcommand{\bbt}{\boldsymbol{\beta}}
\newcommand{\bps}{\boldsymbol{\psi}}

% Initialize knitr and the R session.
<<init, include = FALSE, >>=
library(knitr)
library(ggthemes)
library(grid)
library(gridExtra)
library(ggplot2)

# Set root directory
opts_knit$set(root.dir = "C:/Dropbox/Research/BayesMultCircCovariates")

## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
setwd(opts_knit$get('root.dir'))

source("Spread/Figures/plotBetaLL.R")
source("Code/describeCirc.R")
source("Data/generateCircularGLMData.R")
source("Simulation/simulationStudyCircGLM.R")


# Default ggplot2 theme
myTheme <- theme_bw
@


\section{Introduction}

Circular data is data that is measured in angles or directions. They are frequently encountered in scientific fields as diverse as Molecular Biology, Cognitive Psychology, Political Science \citep{gill2010} and Earth Sciences. Circular data differ from linear data in the sense that circular data are measured in a periodical sample space. For example, an angle of $1^{\circ}$ is quite close to an angle $359^{\circ},$ although linear intuition would suggest otherwise.

Because of this, linear models may not describe the process that has generated the circular data of interest properly. Circular data analysis has been developed to deal with this, but the attention to this type of data has been limited. Only slightly more than a handful of in-depth books on circular data analysis have been published \citep{fisher1995statistical, mardia1999directional, pewsey2013circular}, and statistical methods are somewhat limited.

Here, attention is turned to analysis of datasets with a circular outcome, to be predicted by either continuous (linear) covariates and categorical variables, to provide a model that has both multiple regression and ANCOVA as a special case.

Three main approaches to circular data analysis might be distinguished. First, the intrinsic approach, which uses distribution directly defined on the circle for inference \citep{fisher1992regression}. Second, the wrapping approach, which 'wraps' a univariate distribution around the circle by taking the modulus of data on the real line \citep{coles1998inference, ferrari2009wrapping}. Third, the embedding approach, obtained by projecting points from a bivariate distribution to the circle \citep{nunez2011bayesian, nunez2014bayesian, Hernandez-Stumpfhauserunpublishedmanuscript}. While the wrapping and embedding approach provide promising avenues of study in their own right, here attention is restricted to the intrinsic approach, as it might provide the most natural analysis of circular data.

Within the intrinsic approach, the circular analogue to the normal distribution is the von Mises distribution \citep{von1918ganzzahligkeit}. This symmetric unimodal distribution is given by
\begin{equation}
\mathcal{M}(\theta \vert \mu, \kappa) = \left[ 2 \pi I_0(\kappa) \right]^{-1}
\exp \left( \kappa \cos \left[ \theta - \mu \right] \right),
\end{equation}
where \( \theta \) represents an angular measurement, \( \mu \) represents the mean direction, \( \kappa \) is a concentration parameter, and $I_0$ represents the modified Bessel function of the first kind and order zero. Frequentist methods for these situations have been developed in a circular ANOVA \citep{harrison1986analysis,harrison1988development}, circular ANCOVA \citep{artes2008hypothesis} and circular regression \citep{fisher1992regression}.

Here, a Bayesian analysis of such models will be provided. Early approaches to Markov chain Monte Carlo (MCMC) sampling for the von Mises distribution provide a method for sampling \( \mu \) when \( \kappa \) is known \citep{mardia1976bayesian} and sampling both parameters for a single group of data \citep{damien1999fullbayes}. \citet{guttorp1988finding} present a conjugate prior for the von Mises model. Recent theoretical work has much improved the efficiency of the sampling of the concentration parameter of the von Mises distribution \citep{forbes2014fast}. However, none of these approaches provides association of circular outcomes to linear covariates, comparison of groups of data, or hypothesis testing.

Some development has taken place in the field of semiparametric inference for circular data models as well, generally using Dirichlet process priors \citep{Bhattacharya2009, ghosh2003semiparametric, george2006semiparametric, mcvinish2008semiparametric}. In particular, \citet{ghosh2003semiparametric} provide Bayes Factors for the simple hypothesis test of equality of two means.

A fully Bayesian circular data regression analysis is performed and applied by \citet{gill2010}, using starting values from a frequentist iterative reweighted least squares (IRLS) algorithm, similar to that used by \citet{fisher1992regression}. \citet{gill2010} note that the likelihood of the regression coefficients $\beta$ from their model is not globally concave, and advise careful inspection of the likelihood surface of the regression coefficients. Downsides to the approach taken by \citet{gill2010} are that a prior is not specified, the algorithm is very slow, and it may be uncertain whether the regression coefficients will move to a local maximum instead of the global maximum.

Recent work has provided a multivariate extension of the von Mises distribution \citep{mardia2008multivariate,mardia2014some}, which offers a promising new avenue for circular covariate models. The multivariate von Mises was applied in this context by \citet{lagona2014regression} within a General Linear Model (GLM) setting, applying MCMC likelihood approximation as in \citet{geyer1992constrained} to obtain Maximum Likelihood Estimates (MLEs). This approach is currently not Bayesian, but it is a promising approach because of its flexibility, allowing both the mean and concentration to be dependent on an arbitrary set of covariates, as well as allowing observations to be dependent.

However, there are three main drawbacks to the circular GLM approach currently. First, the GLM approach to circular data analysis is not free from the lack of concavity as described in \citet{gill2010}, although this has not been investigated in detail previously. Second, the the current approach does not have parameters for group mean direction differences, which precludes the popular ANCOVA model. Third, hypothesis tests for this type of model have note been provided, which limits its applicability.

In this paper, the circular data GLM approach will be applied in a fully Bayesian setting. The method presented here builds on previous work, but allows for group mean differences to be incorporated naturally. Furthermore, the lack of concavity in the likelihoods will be examined, and suggestions will be formulated on how to deal with this issue. Then, hypothesis tests for this model using Bayes Factors will be introduced. These tests will then be applied to a practical example from psychology. Finally, simulations into the performance of these methods will be discussed.









\section{Bayesian circular GLM}

Consider a dataset \( (\theta_i, \bx_i, \bd_i), i = 1, \dots n \), where \( \theta_i \in [-\pi, \pi) \) is a circular outcome variable, \( \bx_i \in \mathbb{R^K} \) is a column vector of continuous linear covariates which are assumed to be standardized, and \( \bd_i \in \{0, 1\}^J \) is a column vector of dichotomous variables indicating group membership, which may be dummy variables. Assume that each observed angle $\theta$ is generated from an independent VM distribution \( M(\theta_i \vert \mu_i, \kappa) \). Then, \( \mu_i \) is chosen to be
\begin{equation}
\mu_i = \beta_0 + \bdt^T \bd_i + g(\bbt^T \bx_i),
\end{equation}
where \( \beta_0 [-\pi, \pi) \) is an offset parameter similar which can be seen as a circular intercept, \( \bdt = \delta_1, \dots, \delta_J \) is a vector of circular group difference parameters, \( g(\cdot) : \mathbb{R} \rightarrow \mathbb{S} \) is a twice differentiable link function as described in \citet{fisher1992regression}, and \(\bbt = \beta_1, \dots, \beta_K \) is a vector of regression coefficients.

This model specification differs from the usual approach to circular regression models, as these generally set \( \mu_i = \beta_0 + g(\bbt^T \bx_i)\) \citep{fisher1992regression, gill2010, lagona2014regression}. However, this approach does not work well with dichotomous predictors. To see this, suppose a dichotomous predictor \( j \) is added to an otherwise continuous set of predictors \( \bx_i \). Then, \( \mu_i = \beta_0 + g(\bbt^T \bx_i + \beta_j x_{ij} ) \). However, because the link function will generally not be linear, it will not have \( g(x + c) = g(x) + c \), which suggests that mapping of the other covariates to the circle will change by this constant addition. The way the predicted values change as a result of this is shown in pricThis is undesirable if the effects of the covariates are assumed to be equal for all groups, which is often the case. In addition, this suggests that the link function and therefore the results of the analysis depend on the way the dichotomous variables are labeled, which should not be the case.

\begin{figure}
\label{parallel}
\begin{subfigure}[b]{0.5\textwidth}
<<nonparallel>>=
n <- 100
truedelta <- 2; truebeta <- .4; truekappa <- 20
dat <- as.data.frame(generateCircGLMData(n=n, nconpred = 1, ncatpred = 1, truebeta0 = 0,
                                         truedelta = truedelta, truebeta = truebeta,
                                         residkappa = truekappa))

res1 <- circGLM(th = dat$th, X = dat[,-1], returnPostSample = FALSE, skipDichSplit = TRUE)
plot.predict.circGLM(res1, groupingInBeta = TRUE)
@
\caption{$\mu_i = \beta_0 + g(\beta x_i  + \delta d_i)$}
\end{subfigure}
~
\begin{subfigure}[b]{0.5\textwidth}
<<parallel>>=
res2 <- circGLM(th = dat$th, X = dat[,-1], returnPostSample = FALSE)


pdf("ExampleCircularLinearPlot.pdf")
plot.predict.circGLM(res2, xlab = "Predictor X", ylab = "Circular Outcome")
dev.off()
@
\caption{$\mu_i = \beta_0  + \delta d_i + g(\beta x_i)$}
\end{subfigure}
\caption{Prediction lines from two different models, which were fitted to a dataset with $n = 100$, and true parameters $\delta = \Sexpr{truedelta}, \beta_0 = \pi/2, \beta = \Sexpr{truebeta}, \kappa = \Sexpr{truekappa}$. The two models have (a) dichotomous predictors placed in the link function, and (b) dichotomous predictors treated separately.}
\end{figure}

The approach taken here is most similar to the parallel case described by \citet{artes2008hypothesis}. In that approach, each group is given a specific intercept. However, the approach here is more flexible in that it allows a researcher to easily specify a factorial design with main effects only, by specifying the factors as dichotomous variables. \citet{artes2008hypothesis} also describes a non-parallel case, which can be obtained as a special case of the model provided here by the appropriate including interaction terms to the model in \( \bx_i \).

\subsection{Likelihood}

The joint likelihood is given by
\begin{align}
L(\beta_0, \bbt, \kappa \vert \bt, \bX) &\propto  \prod_{i=1}^{n} M(\theta_i \vert \mu_i, \kappa) \\
&=  \left\lbrace 2 \pi I_0(\kappa) \right\rbrace^{-n} \exp \left\lbrace \kappa \sum_{i=1}^{n} \cos \left[ \theta_i - \beta_0 - \bdt^T \bd_i - g(\bbt^T \bx_i)\right]  \right\rbrace.
\end{align}

It can be shown (see Appendix \ref{beta0}) that the conditional distribution of \( \beta_0 \) is \( M(\bar\psi, R_{\psi} \kappa), \) where \( \bar\psi \) and \( R_\psi \) are the mean direction and resultant length of the vector \( \bps = (\psi_1, \dots, \psi_n),\) with \( \psi_i = \theta_i - \mu_i ~  \forall ~ i = 1, \dots, n.\)   Conditionals for \( \kappa \) and \( \boldsymbol\beta \) are not of simple form and require special attention. \label{condbeta0}


\subsection{Priors}

The next step in the model specification is setting prior distributions for the parameters.

For the von Mises part of the model, we follow the conjugate prior provided by \citet{guttorp1988finding}. It can be seen that the parameters \( (\beta_0, \kappa) \) follow the conjugate prior when we imagine \( \psi_i = \theta_i - \mu_i \).
\( M(\theta_i \vert \mu_i, \kappa) \)

The prior for \( \bbt \) can be taken to be improper and proportional to constant. However, this causes likelihood problems. Therefore, we take a normal prior.

The prior for \( \bdt \) is taken to be uniform.






\section{MCMC sampling}

In this section, details on the MCMC sampling procedure will be discussed.

\subsection{Sampling $\beta_0$}

As discussed in Section \ref{condbeta0},

\[ \beta_0 \mid \bbt, \bdt, \kappa, \bt, \bd, \bX \sim  M(\bar\psi, R_{\psi} \kappa).\]

In each iteration, a new vector \( \bps \) is obtained using the current values of $\mu_i$, and corresponding values of \( \bar\psi, \) and \( R_{\psi} \) are computed. Then, it is straightforward to sample from \( \beta_0 \) from the von Mises distribution, for example as in \citet{fisher1995statistical}.

\subsection{Sampling $\bbt$}

The posterior for some \( \beta_j \), conditional on all other parameters (denoted by \( \mid \cdot \)), is given by

\[ f(\beta_j \mid \cdot ) \propto L (\beta_j \mid \cdot) p (\beta_j) \]

where \( p(\beta_j) \) is the prior distribution for \( \beta_j \). Possible choices include \( p(\beta_j) \propto 1 \) and \( p(\beta_j) \propto N(0, 1).\)

In the current MCMC sampler, this is done by a Metropolis-Hastings random walk. Because a random walk on \( \beta_j \) provided slow convergence

\subsection{Sampling $\bdt$}

It can be seen that the conditional distribution of \( \delta_k \) is a convolution of two von Mises distributions, which itself is not von Mises. \citet[p. 44]{mardia1999directional} provides an approximation for such a convolution. Here, however, a simple Metropolis-Hastings random walk is applied.

\subsection{Sampling $\kappa$}

\section{Hypothesis tests}

The Bayesian approach to testing two discrete hypotheses against each other is by updating our prior odds of the

Here, two types of tests will be considered.

\subsection{Equality constrained hypotheses}

Consider the two hypotheses, \( H_0 : \phi = \phi_0, \) and \( H_1 : \phi \in \Omega_\phi, \) where \( \Omega_\phi \) is a sample space of \( \phi \). The Bayes Factor for








\section{Example}






\section{Simulation study}



%
\appendix
%
% \section{Conditional distribution of $\beta_0$} \label{beta0}
%
% Here, I will show that the conditional distribution for $\beta_0$,
% $$ L(\beta_0 \vert \boldsymbol\beta, \kappa, \theta, \boldsymbol{X}) \propto \exp \left\lbrace \kappa \sum_{i=1}^n \cos \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right]  \right\rbrace \propto \mathcal{VM}(\beta_0 \vert \bar\psi, R_{\psi}\kappa). $$
% The proof for the conditional distribution of $\beta_0$ in the GLM closely follows the derivation for the distribution of the mean direction $\mu$ of the von Mises distribution, which shows that $L(\mu \vert \kappa, \boldsymbol\theta) \propto \mathcal{VM}(\mu \vert \bar\theta, R_{\theta} \kappa)$.
%
% The conditional likelihood of $\beta_0$ is given by
%
% \begin{align*}
% L(\beta_0 \vert \boldsymbol\beta, \kappa, \theta, \boldsymbol{X}) & \propto \exp \left\lbrace \kappa \sum_{i=1}^n \cos \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] \right\rbrace \\
% & = \exp \left\lbrace \kappa \sum_{i=1}^n \cos \left[ \beta_0 - (\theta_i - g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right]  \right\rbrace.
% \end{align*}
%
% We know that for any angle $\psi_i, i = 1, \dots, n$,
%
% $$ C_{\psi} = \sum_{i=1}^n \cos(\psi_i), ~~ S_{\psi} = \sum_{i=1}^n \sin(\psi_i), ~~ R_{\psi} = \sqrt{C_{\psi}^2 + S_{\psi}^2}, ~~ $$
%
% $$ \text{and} ~~ \frac{C_{\psi}}{R_{\psi}} = \cos \bar\psi, ~~ \frac{S_{\psi}}{R_{\psi}} = \sin \bar\psi, ~~ \text{where} ~~ \bar\psi = \text{atan2}(S_{\psi}, C_{\psi}). $$
%
% Thus, setting angle $\psi_i = \theta_i - g(\boldsymbol\beta^T \boldsymbol{x}_i)$,
%
%
% \begin{align*}
% L(\beta_0 \vert \boldsymbol\beta, \kappa, \boldsymbol\theta, \boldsymbol{X}) & \propto \exp \left\lbrace \kappa \sum_{i=1}^n \cos ( \beta_0 - \psi_i)  \right\rbrace \\
% & = \exp \left\lbrace \kappa   \left[ \cos\beta_0 \sum_{i=1}^n \cos\psi_i +  \sin\beta_0 \sum_{i=1}^n \sin\psi_i \right]  \right\rbrace \\
% & = \exp \left\lbrace R_{\psi}  \kappa  \left[ \cos\beta_0 \frac{C_{\psi}}{R_{\psi}} +  \sin\beta_0 \frac{S_{\psi}}{R_{\psi}} \right]  \right\rbrace \\
% & = \exp \left\lbrace R_{\psi} \kappa   \left[ \cos\beta_0 \cos{\bar\psi} +  \sin\beta_0 \sin{\bar\psi}\right]  \right\rbrace \\
% & = \exp \left\lbrace R_{\psi} \kappa \cos \left( \beta_0 - \bar\psi \right)  \right\rbrace \\
% & \propto \mathcal{VM}(\beta_0 \vert \bar\psi, R_{\psi} \kappa).
% \end{align*}
%
%
%



\bibliographystyle{apacite}
\bibliography{C:/Dropbox/LiteratureCircular/CircularData}



\end{document}
