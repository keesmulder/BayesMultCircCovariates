\documentclass[12pt,a4paper]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{fullpage}

\usepackage[]{algorithm2e}

\usepackage{apacite}
\usepackage{natbib}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    citecolor=blue,
    urlcolor=blue
}

\author{Kees Mulder}
\title{A Circular Outcome Bayesian GLM}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle


\tableofcontents


\newpage

% R setup.



\section{Introduction}

Circular data is data that is measured in angles or directions. They are frequently encountered in scientific fields as diverse as Molecular Biology, Cognitive Psychology, Political Science \citep{gill2010} and Earth Sciences. Circular data differ from linear data in the sense that circular data are measured in a periodical sample space. For example, an angle of $1^{\circ}$ is quite close to an angle $359^{\circ},$ although linear intuition would suggest otherwise.

Because of this, linear models may not describe the process that has generated the circular data of interest well. Circular data analysis has been developed to deal with this, but the attention to this type of data has been limited. Only slightly more than a handful of in-depth books on circular data analysis have been published \citep{fisher1995statistical,mardia1999directional,pewsey2013circular}, and statistical methods are generally fairly limited. The Bayesian paradigm, in particular MCMC methods, may provide a flexible approach to analyzing circular data.

The circular analogue to the normal distribution is the von Mises distribution \citep{von1918ganzzahligkeit}. This symmetric unimodal distribution is given by
\begin{equation}
\mathcal{VM}(\theta \vert \mu, \kappa) = \left[ 2 \pi I_0(\kappa) \right]^{-1}
\exp \left( \kappa \cos \left[ \theta - \mu \right] \right),
\end{equation}
where $\theta$ represents an angular measurement, $\mu$ represents the mean direction, $\kappa$ is a concentration parameter, and $I_0$ represents the modified Bessel function of the first kind and order zero. Early approaches to Markov chain Monte Carlo (MCMC) sampling for this model provide a method for sampling $\mu$ when $\kappa$ is known \citep{mardia1976bayesian} and sampling both parameters for a single group of data \citep{damien1999fullbayes}. Recent theoretical work has much improved the efficiency of the sampling of the concentration parameter of the von Mises distribution \citep{forbes2014fast}. None of these approaches allows prediction of circular outcome data or comparison of groups. Frequentist methods for these situations have been developed in a circular ANOVA \citep{harrison1986analysis,harrison1988development} and circular regression \citep{fisher1992regression}, but they are limited in scope and applicability.

A Bayesian circular data regression analysis is performed and applied by \citet{gill2010}, using starting values from a frequentist iterative reweighted least squares (IRLS) algorithm, similar to that used by \citet{fisher1992regression}. In that paper, the authors show that the likelihood of the regression coefficients $\beta$ from their model is not globally concave, and advise careful inspection of the likelihood surface of the regression coefficients. Downsides to the approach taken by \citet{gill2010} are that a prior is not specified, the algorithm is very slow, and it may be uncertain whether the regression coefficients will move to a local maximum instead of the global maximum.

Recent work has provided a multivariate extension of the von Mises distribution \citep{mardia2008multivariate,mardia2014some}. \citet{lagona2014regression} extends this to the General Linear Model (GLM) setting, applying MCMC likelihood approximation as in \citet{geyer1992constrained} to obtain Maximum Likelihood Estimates (MLEs). This approach is not fully Bayesian, but it is a promising approach because of its flexibility, allowing both the mean and concentration to be dependent on an arbitrary set of covariates, as well as allowing observations to be dependent.

In this paper, a novel fully Bayesian method for the analysis of circular data will be developed. Its application will be made accessible, and potential issues will be studied. The GLM approach to analysis of circular data is not free from the lack of concavity as described in \citet{gill2010}, although this has not been investigated in literature. In this paper, the GLM approach will be applied in a fully Bayesian setting. Furthermore, the lack of concavity in the likelihoods will be examined, and suggestions will be formulated on how to deal with this issue. Finally, simulations into its performance will be discussed.

\section{The model}

Let $\boldsymbol\theta = \theta_1, \dots, \theta_n$ be a set of circular data, where each $\theta_i \in [-\pi, \pi)$, and let $\boldsymbol{X} = \boldsymbol{x}_1, \dots, \boldsymbol{x}_n$ be an $(n \times K)$ matrix of linear predictors. Assume that each observed angle $\theta$ is generated from an independent VM distribution $ VM(\theta_i \vert \hat{\theta_i}, \kappa)$. The predicted value $\hat{\theta_i}$ is given by
\begin{equation}
\hat{\theta_i} = \beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i),
\end{equation}
where $\beta_0$ is an offset parameter, $g(\cdot)$ is a link function, as described in \citet{fisher1992regression}, and $\boldsymbol{\beta} = \beta_1, \dots, \beta_K$ is a vector of coefficients.

\subsection{Link function}

The link function maps $\boldsymbol\beta^T \boldsymbol{x}_i \in \mathbb{R}$ to the circular sample space $\mathbb{S}^1$.

A popular choice is
\begin{equation}
g(x) = 2 \tan^{-1}(x),
\end{equation} because for this link function $g(0) = 0,$ and because its range is $\mathbb{S}^1$, which allows the predicted values across the entire circle. This link function is shown in figure \ref{linkfunc}. To ensure that all , all predictors must be standardized.

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\linewidth]{figure/Link-1} 

}



\end{knitrout}
\caption{Link function $g(x) = 2 \tan^{-1}(x).$}
\label{linkfunc}
\end{figure}

\subsection{Likelihood}

The joint likelihood is given by
\begin{align}
L(\beta_0, \boldsymbol{\beta}, \kappa \vert \boldsymbol{\theta}, \boldsymbol{X}) &\propto  \prod_{i=1}^{n} \mathcal{VM}(\theta_i \vert \beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i), \kappa) \\
&=  \left\lbrace 2 \pi I_0(\kappa) \right\rbrace^{-n} \exp \left\lbrace \kappa \sum_{i=1}^{n} \cos \left[ \theta_i - \beta_0 - g(\boldsymbol\beta^T \boldsymbol{x}_i) \right]  \right\rbrace.
\end{align}

It can be shown (see Appendix A) that the conditional distribution of $\beta_0$ is proportional to $\mathcal{VM}(\beta_0 \vert \bar\psi, R_{\psi} \kappa).$ This makes it straightforward to sample from $\beta_0$ as in \citet{fisher1995statistical}. Conditionals for $\kappa$ and $\boldsymbol\beta$ are not of simple form and require special attention.

\subsection{Prior}

For $\beta_0$ and $\kappa$, a conjugate prior will be used. Setting $\psi_i = \theta_i - g(\boldsymbol\beta^T \boldsymbol{x}_i)$ leaves us with simply the regular von Mises likelihood $L_{\mathcal{VM}}(\beta_0, \kappa \vert \boldsymbol\psi)$, where $\beta_0$ is the mean direction. \citet{guttorp1988finding} present a conjugate prior for the von Mises distribution. Applied to this situation, it is given up to a constant of proportionality by
\begin{equation}
p(\mu_{\psi}, \kappa) \propto  I_0 (\kappa) ^{-c} \exp\{R_0 \kappa \cos (\mu_{\psi} - \mu_0)\},
\end{equation}
where $\mu_{\psi}$ represents the mean direction of the set of angles $\boldsymbol\psi$,  which represents $c$ observations of $\psi$ with prior mean direction $\mu_0$ and prior resultant length $R_0$. Note, however, that in this case this represents our prior information regarding the distribution of $\psi$ rather than $\theta$. Researchers may or may not be able to formulate prior expectations exist about $\mu_{\psi}$ and $\kappa$. Regardless, this method allows straightforward specification of a non-informative prior by setting $R_0 = 0$ and $c = 0$.

For each $\beta$, a normal prior will be specified as $\beta \sim N(0, 10)$.



\subsection{Conditional likelihood and posterior of $\beta$}



The conditional log-likelihood of $\boldsymbol{\beta}$ is % independent of $\kappa$ and
given by
\begin{align}
%L(\boldsymbol{\beta} \vert \beta_0, \boldsymbol{\theta}, \boldsymbol{X}) &\propto
%\exp \left\lbrace \sum_{i=1}^{n} \cos \left[ \theta_i - \beta_0 - g(\boldsymbol\beta^T \boldsymbol{x}_i) \right]  \right\rbrace. \\
\ell (\boldsymbol{\beta} \vert \beta_0, \boldsymbol{\theta}, \boldsymbol{X}) &\propto
\sum_{i=1}^{n} \cos \left[ \theta_i - \beta_0 - g(\boldsymbol\beta^T \boldsymbol{x}_i) \right].
\end{align}
%For simplicity, we will consider the unidimensional case here.

Due to the periodical nature of the outcome variable $\theta$, the conditional log-likelihood of some $\beta$ may have not be globally concave and may have more than one maximum. Consider a simple dataset, shown in Figure \ref{SimpleDataset}, with a single predictor $\boldsymbol{X} = \left\lbrace -3, -2, \dots, 2, 3 \right\rbrace $ that is standardized, and $\theta_i = \pi + 2 \tan^{-1} (x_i) + \varepsilon,$ for each $ i \in 1, 2, \dots, 7,$ with $\varepsilon \sim \mathcal{N}(0, .1)$. Then, true $\beta_0 = \pi$ and true $\beta = 1$.

\begin{figure}
\label{SimpleDataset}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/SimpleDataset-1} 

}



\end{knitrout}
\caption{Relationship between the predictor $X$ and circular outcome $\theta$ in a simple case.}
\label{SimpleDataset}
\end{figure}

The shape of the log-likelihood of $\beta$, conditional on $\beta_0$, is shown in Figure \ref{SimpleDataBeta}. Even in this basic case, the shape of the conditional likelihood is irregular, and depends heavily on the current value of $\beta_0$. Note that if the current $\beta_0$ is close to the true value $\pi$, the maximum of the log-likelihood is close to 1, the true value for $\beta$. However, if $\beta_0$ is 0, the maximum of the log-likelihood is close to -1.

\begin{figure}
\label{SimpleDataBeta}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/SimpleDataBeta-1} 

}



\end{knitrout}
\caption{Shape of the conditional likelihood for three different values for $\beta_0$.}
\label{SimpleDataBeta}
\end{figure}

Another example dataset is given in Figure \ref{ZeroBData}, with $n = 30$, true $\beta = 0$, true $\beta_0 = \pi$ and standard normal errors. Here, we see that with $\beta_0 = 0$, the conditional log-likelihood has a minimum at the true $\beta$ at 0, with two asymptotes extending infinitely outwards. The lack of global concavity is not present in the posterior when using a normal prior (here, $N(0, 1)$, as seen in Figure \ref{ZeroBBetaNormalPrior}. Regardless, the irregular shape of the posterior still complicates inference.

\begin{figure}
\label{ZeroBData}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/ZeroBData-1} 

}



\end{knitrout}
\caption{Relationship between the predictor $X$ and circular outcome $\theta$ in a simple case.}
\end{figure}



\begin{figure}
\label{ZeroBBeta}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/ZeroBBeta-1} 

}



\end{knitrout}
\caption{Shape of the conditional likelihood for three different values for $\beta_0$.}
\end{figure}

\begin{figure}
\label{ZeroBBetaNormalPrior}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/ZeroBBetaNormalPrior-1} 

}



\end{knitrout}
\caption{Shape of the conditional posterior for three different values for $\beta_0$.}
\end{figure}

%' \begin{figure}
%' \label{cat1}
%' <<CatData>>=
%'
%' set.seed(212)
%' n <- 100
%' X  <- sample(0:1, n, replace=TRUE)
%' bt <- 1
%' th <- pi + atanLF(bt*X, 2) + rnorm(n, 0, .4)
%' d <- data.frame(th=th, X=X)
%' ggplot(aes(x=X, y=th), data=d) + geom_point() + ylab(expression(theta))  + ylim(0, 2*pi) + myTheme()
%'
%' meanDir(th[X==0])
%' @
%'
%' <<CatBetaPlot>>=
%' res <- 200
%' b0cur <- 0
%' xl <- 5 * c(-1, 1)
%' ll1p1 <- plotbeta(normalPrior=FALSE, res=res, xl=xl, b0=0,
%'                   kp=1, r=4, th=th, X=as.matrix(X)) +
%'   myTheme() + labs(title=expression(paste(beta[0], " = 0")))
%' ll2p1 <- plotbeta(normalPrior=FALSE, res=res, xl=xl, b0=pi/2,
%'                   kp=1, r=2, th=th, X=as.matrix(X)) +
%'   myTheme() + labs(title=expression(paste(beta[0], " = ", pi / 2)))
%' ll3p1 <- plotbeta(normalPrior=FALSE, res=res, xl=xl, b0=pi,
%'                   kp=1, r=2, th=th, X=as.matrix(X)) +
%'   myTheme() + labs(title=expression(paste(beta[0], " = ", pi)))
%' grid.arrange(ll3p1, ll2p1, ll1p1, ncol=3)
%' @
%' \caption{Shape of the conditional posterior for three different values for $\beta_0$.}
%' \end{figure}


\section{MCMC Sampling}

This section will provide some details as to the proposed MCMC-algorithm, given in Algorithm \ref{basealg}. The algorithm separates the sampling of $(\beta_0, \kappa)$ and $\boldsymbol\beta$ by calculating $\boldsymbol\psi$ and using $\boldsymbol\psi$ in obtaining $(\beta_0, \kappa)$. Sampling of $\kappa$ employs an algorithm discussed by \citet{forbes2014fast} and will not be repeated here.

Becayse of the irregular shape of the likelihood, several
A random walk mechanism can be employed for $\boldsymbol\beta$. This requires setting some bandwith, which may be considered a downside of this algorithm. We might suggest using a default normal proposal for $\beta$, because the expected area of the parameter space of $\beta$ will be known.


\subsection{Algorithm}
% \begin{algorithm}[H]
%
% \texttt{For each $\beta_j$, select an appropriate bandwith $b_j.$}
%
% \texttt{Set $\boldsymbol{\beta}^{(1)}$ and $\kappa^{(1)}$ to their starting values.}
%
%  $m = n + c.$
%
%  \For{$i \leftarrow 2$ \KwTo $Q$}{
%
%   $\psi_i = \theta_i - g (\boldsymbol{\beta}^T \boldsymbol{x}_i),$ for each $i = 1, \dots, n.$
%
%   Compute $\bar\psi$ and $R_{\psi}.$
%
%   Draw $\beta_0^{(i)} \sim \mathcal{VM}(\bar\psi, R_{\psi} \kappa).$
%
%   $h = - R_\psi  \cos\left(\beta_0 - \bar{\psi}\right).$
%
%   Sample $\kappa$ as in \citet{forbes2014fast}, using $(h, m).$
%
%   \For{$j \leftarrow 1$ \KwTo $K$}{
%
%     Draw $\tau_j \sim U(-b_j, b_j).$
%
% 	  $\beta_j^{(can)} = \beta_j^{(i-1)} + \tau_j.$
%
% 	  Compute
% 	  \begin{align*}
% 	  A_{\beta_j} = ~& \ln L \left( \beta_{k}^{(can)} \vert \beta_0^{(i)}, \boldsymbol\beta_{-(j)}^{(i-1)}, \kappa^{(i)}, \boldsymbol\theta, \boldsymbol{X} \right) + \ln \text{Prior}_{\beta_j}\left(\beta_{j}^{(can)}\right) \\
% 	  - & \ln L \left( \beta_{j}^{(i-1)} \vert \beta_0^{(i)}, \boldsymbol\beta_{-(j)}^{(i-1)}, \kappa^{(i)}, \boldsymbol\theta, \boldsymbol{X} \right) - \ln \text{Prior}_{\beta_j}\left(\beta_{j}^{(i-1)}\right).
% 	  \end{align*}
%
% 	  Draw $u_j \sim U(0,1).$
%
%   	  \eIf{$ A_{\beta_j} > \ln u_j$} {
%   	    $\beta_j^{(i)} = \beta_j^{(can)}.$
%   	  }{
%   	    $\beta_j^{(i)} = \beta_j^{(i-1)}.$
%   	  }
%
% }
%
%   }
%  \caption{An algorithm for the given GLM approach}
%  \label{basealg}
% \end{algorithm}
%


\begin{algorithm}[H]

% \texttt{For each $\beta_j$, select an appropriate bandwith $b_j.$}

 \texttt{Set $\boldsymbol{\beta}^{(1)}$ and $\kappa^{(1)}$ to their starting values.}

  $m = n + c.$

 \For{$i \leftarrow 2$ \KwTo $Q$}{

  $\psi_i = \theta_i - g (\boldsymbol{\beta}^T \boldsymbol{x}_i),$ for each $i = 1, \dots, n.$

  Compute $\bar\psi$ and $R_{\psi}.$

  Draw $\beta_0^{(i)} \sim \mathcal{VM}(\bar\psi, R_{\psi} \kappa).$

  $h = - R_\psi  \cos\left(\beta_0 - \bar{\psi}\right).$

  Sample $\kappa$ as in \citet{forbes2014fast}, using $(h, m).$

  Draw $\boldsymbol\beta$.

  }
 \caption{An outline of the algorithm for the given GLM approach}
 \label{basealg}
\end{algorithm}


\begin{itemize}
\item
\end{itemize}

%
% \section{The model}
%
% \begin{itemize}
% \item Multivariate von Mises distribution
% \item Other multivariate circular distributions?
% \item Notation
% \item Link function
% \item Likelihood
% \item Prior
% \item Posterior
% \end{itemize}
%
% \section{Computational issues}
%
% \begin{itemize}
% \item Shape of the marginal likelihood for some common cases
% \item Selection of starting values
% \item Non-interpretability of beta's
% \item Solutions
% \end{itemize}


\subsection{Limitations}

\begin{itemize}
\item The current method does not work well when the sum of coefficients becomes too large.
\item
\end{itemize}

\section{Simulation study}

\subsection{Setup}

\begin{itemize}
\item Show that the sampler converges usually with reasonable models
\item Show the extent to which the sampler still works, and when it doesn't in a limited number of edge cases.
\end{itemize}

\subsubsection{One continuous, one categorical predictor}

\subsubsection{Two continuous, two categorical predictors}

\subsection{Results}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
## Simulation study results with number of simulations: . Iterations per dataset Q: .
## Range of the link function is r*pi, here r = . Using a  prior for Beta.
## Reparametrization was performed.
##  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
## 
## (1) Betadesign: l = 0.05
##     n kp b0_meandir b0_in_CCI kp_mean kp_mode kp_in_HDI bt_1_mean
## 1  20  1        1.6      1.00    0.73    0.40      0.97    -0.041
## 2  50  1        1.6      0.99    0.76    0.71      0.84    -0.010
## 3 100  1        1.6      0.98    0.93    0.93      0.91     0.019
## 4  20  4        1.6      0.96    4.33    3.90      0.94     0.029
## 5  50  4        1.6      0.95    4.24    4.08      0.96     0.049
## 6 100  4        1.6      0.95    4.11    4.03      0.97     0.052
## 7  20 32        1.6      0.94   39.46   35.57      0.94     0.049
## 8  50 32        1.6      0.95   34.76   33.46      0.96     0.050
## 9 100 32        1.6      0.94   33.06   32.43      0.95     0.050
##   bt_1_in_CCI
## 1        1.00
## 2        1.00
## 3        1.00
## 4        0.98
## 5        0.95
## 6        0.93
## 7        0.89
## 8        0.88
## 9        0.87
\end{verbatim}
\end{kframe}
\end{knitrout}




\begin{itemize}
\item Cases
\item Results
\item Figures
\end{itemize}

\section{Application}

To show how to apply this methodology in practice, we will here show how a dataset could be analyzed.

\subsection{Problem}

\begin{itemize}
\item 1-3 example datasets, analyzed with CCI's etc.
\end{itemize}

\subsection{Estimation}

\begin{itemize}
\item Show how MCMC sampler can be applied
\item Explain the output of the sampler clearly
\item Draw clear-cut conclusions from this
\end{itemize}

\subsection{Model fit}

\begin{itemize}
\item Show how IC's and ppc can be applied
\item Draw clear-cut conclusions from this
\end{itemize}

\subsection{Model comparison}

\begin{itemize}
\item Show how BF's are used
\item Draw clear-cut conclusions from this
\end{itemize}

\section{Discussion}

\begin{itemize}
\item This model is great.
\item This model is far superior to the projected approach, because of reasons. Or maybe worse.
\item Can be extended with association matrix, multilevel structures (Lagona).
\end{itemize}







\appendix

\section{Conditional distribution of $\beta_0$} \label{beta0}

Here, I will show that the conditional distribution for $\beta_0$,
$$ L(\beta_0 \vert \boldsymbol\beta, \kappa, \theta, \boldsymbol{X}) \propto \exp \left\lbrace \kappa \sum_{i=1}^n \cos \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right]  \right\rbrace \propto \mathcal{VM}(\beta_0 \vert \bar\psi, R_{\psi}\kappa). $$
The proof for the conditional distribution of $\beta_0$ in the GLM closely follows the derivation for the distribution of the mean direction $\mu$ of the von Mises distribution, which shows that $L(\mu \vert \kappa, \boldsymbol\theta) \propto \mathcal{VM}(\mu \vert \bar\theta, R_{\theta} \kappa)$.

The conditional likelihood of $\beta_0$ is given by

\begin{align*}
L(\beta_0 \vert \boldsymbol\beta, \kappa, \theta, \boldsymbol{X}) & \propto \exp \left\lbrace \kappa \sum_{i=1}^n \cos \left[ \theta_i - (\beta_0 + g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right] \right\rbrace \\
& = \exp \left\lbrace \kappa \sum_{i=1}^n \cos \left[ \beta_0 - (\theta_i - g(\boldsymbol\beta^T \boldsymbol{x}_i)) \right]  \right\rbrace.
\end{align*}

We know that for any angle $\psi_i, i = 1, \dots, n$,

$$ C_{\psi} = \sum_{i=1}^n \cos(\psi_i), ~~ S_{\psi} = \sum_{i=1}^n \sin(\psi_i), ~~ R_{\psi} = \sqrt{C_{\psi}^2 + S_{\psi}^2}, ~~ $$

$$ \text{and} ~~ \frac{C_{\psi}}{R_{\psi}} = \cos \bar\psi, ~~ \frac{S_{\psi}}{R_{\psi}} = \sin \bar\psi, ~~ \text{where} ~~ \bar\psi = \text{atan2}(S_{\psi}, C_{\psi}). $$

Thus, setting angle $\psi_i = \theta_i - g(\boldsymbol\beta^T \boldsymbol{x}_i)$,


\begin{align*}
L(\beta_0 \vert \boldsymbol\beta, \kappa, \boldsymbol\theta, \boldsymbol{X}) & \propto \exp \left\lbrace \kappa \sum_{i=1}^n \cos ( \beta_0 - \psi_i)  \right\rbrace \\
& = \exp \left\lbrace \kappa   \left[ \cos\beta_0 \sum_{i=1}^n \cos\psi_i +  \sin\beta_0 \sum_{i=1}^n \sin\psi_i \right]  \right\rbrace \\
& = \exp \left\lbrace R_{\psi}  \kappa  \left[ \cos\beta_0 \frac{C_{\psi}}{R_{\psi}} +  \sin\beta_0 \frac{S_{\psi}}{R_{\psi}} \right]  \right\rbrace \\
& = \exp \left\lbrace R_{\psi} \kappa   \left[ \cos\beta_0 \cos{\bar\psi} +  \sin\beta_0 \sin{\bar\psi}\right]  \right\rbrace \\
& = \exp \left\lbrace R_{\psi} \kappa \cos \left( \beta_0 - \bar\psi \right)  \right\rbrace \\
& \propto \mathcal{VM}(\beta_0 \vert \bar\psi, R_{\psi} \kappa).
\end{align*}





\bibliographystyle{apacite}
\bibliography{C:/Dropbox/LiteratureCircular/CircularData}

\end{document}
